# ğŸ§ VisionAI-Audio â€” Whisper Model Evaluation & Comparison

A complete research-grade benchmark suite for **evaluating Whisper-based speech-to-text models** across multiple languages, noise levels, and audio qualities.  
This phase of the project (Task 2) focuses on **model evaluation, metric analysis, and visualization** to determine the most accurate and efficient transcription system.

---

## ğŸ§­ Overview

This module compares multiple **Whisper variants** â€” including open-source and optimized implementations â€” using diverse real-world audio datasets.  
It establishes a **standardized evaluation pipeline** for accuracy, speed, and resource efficiency.

**Models Evaluated**
- ğŸ§© **OpenAI Whisper (Large-v3)** â€” Baseline reference  
- âš¡ **Faster-Whisper** â€” Optimized CTranslate2 version  
- ğŸ’» **Whisper.cpp** â€” Lightweight C++ implementation  
- ğŸ” **WhisperX** â€” Alignment-enhanced variant  

---

## ğŸ¯ Objectives
- Evaluate transcription accuracy and efficiency across different models  
- Measure performance under varied audio conditions (clean â†’ noisy, short â†’ long)  
- Generate unified reports with **WER**, **CER**, and performance charts  
- Identify trade-offs between accuracy, inference speed, and system load  

---

## ğŸ“Š Evaluation Metrics

| Category | Metrics |
|-----------|----------|
| **Accuracy** | Word Error Rate (WER), Character Error Rate (CER), Language-specific accuracy (English & Urdu) |
| **Speed** | Transcription time, Cold vs Warm start latency |
| **Resource Usage** | CPU %, GPU VRAM, RAM consumption |
| **Robustness** | Performance on noisy audio, multiple speakers, accent recognition |
| **Scenarios Tested** | Clean speech, street noise, office background, Urdu/English code-switching |

---

## ğŸ§© Repository Structure

```
VisionAI-Audio/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ model_comparison.py              # Core batch-comparison logic
â”‚   â”œâ”€â”€ metrics_calculator.py           # WER/CER computation
â”‚   â”œâ”€â”€ resource_monitor.py             # CPU/GPU/RAM tracking
â”‚   â””â”€â”€ transcription_benchmark.py      # Automated metrics runner
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ model_comparison_analysis.ipynb # Visualization & statistical analysis
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ faster_whisper/
â”‚   â”œâ”€â”€ whisper_cpp/
â”‚   â”œâ”€â”€ whisperx/
â”‚   â””â”€â”€ model_configs.yaml
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ reports/                        # JSON metric outputs & PDF/HTML summaries
â”‚   â””â”€â”€ visualizations/                # Charts & plots from analysis
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ model_evaluation.md            # Detailed metric and methodology guide
â”‚   â””â”€â”€ api_documentation.md
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_audio_samples/            # Evaluation dataset (English & Urdu)
â”‚   â””â”€â”€ test_model_comparison.py
â”‚
â””â”€â”€ README.md
```
## âš™ï¸ How It Works

1. **Generate Transcripts**  
   Each model transcribes the same set of audio samples.  
   The output JSONs are saved under:  
   `results/reports/{model_name}_results.json`

2. **Evaluate Metrics**  
   `metrics_calculator.py` computes WER & CER by comparing each model to the **OpenAI Whisper baseline**.

3. **Benchmark Runner**
   ```bash
   python code/transcription_benchmark.py
   ```
   â†’ Produces metrics JSONs for all models and saves them in `results/reports/`

---

## ğŸ“Š Visual Analysis

Open the notebook:

```bash
notebooks/model_comparison_analysis.ipynb
```

To view bar charts, line graphs, and heatmaps for accuracy vs speed vs resources.

---

## ğŸ“ˆ Outputs & Reports

By the end of Task 2, the system produces:

- 4 model transcription JSONs  
- 3 metric comparison JSONs  
- 1 combined summary JSON  
- 1 visualization notebook    

All outputs are stored under:

- `results/reports/`  

---

## ğŸ§ª Testing & Validation

- â‰¥ 20 diverse audio samples per model  
- Mix of English & Urdu speech  
- Varied lengths (30 s â†’ 15 min) and noise levels  
- Automated WER/CER calculation  
- Resource usage logged via `resource_monitor.py`

---

## ğŸ“š Documentation

See `docs/model_evaluation.md` for:

- Metric definitions & formulas  
- Evaluation methodology  
- Interpretation guide for results  
- Model selection recommendations

---

## ğŸ Deliverables Summary

| Deliverable                                                                 | Status |
|-----------------------------------------------------------------------------|--------|
| 4 Whisper models tested (Faster-Whisper, Whisper.cpp, WhisperX, OpenAI Whisper) | âœ…     |
| 20+ audio samples evaluated per model                                       | âœ…     |
| Complete WER/CER metrics & visual reports                                   | âœ…     |
| Evaluation scripts & notebook completed                                     | âœ…     |
| Documentation & README updated                                              | âœ…     |
| Integration ready for Flask pipeline (Task 3)                               | ğŸ”„     |

---

## ğŸ‘¥ Contributors

**VisionAI Team** â€” R&D on Automatic Speech Recognition and Model Evaluation  
Maintained by **Talha**

---

## ğŸ—“ï¸ Project Timeline

**Task 2: Model Evaluation & Comparison**  
**Deadline:** 15 Oct 2025 âœ… Completed

  
